{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01421000",
   "metadata": {},
   "source": [
    "# Exercise-3: Regularization with Alpha Dropout and MC Dropout\n",
    "\n",
    "Using the MNIST dataset, extend the previously trained deep neural network by applying Alpha Dropout. Then, without retraining, use Monte Carlo (MC) Dropout at inference to estimate if you can achieve better accuracy. Set random seeds to 42. Use the following configuration:\n",
    "\n",
    "- Flatten input images to 28 × 28 = 784 features\n",
    "- 3 hidden layers, 64 neurons each\n",
    "- SELU activation function (required for Alpha Dropout)\n",
    "- LeCun normal initialization\n",
    "- Alpha Dropout rate: 0.1 in all hidden layers\n",
    "- Output layer: 10 neurons with softmax\n",
    "- Optimizer: Nadam\n",
    "- learning_rate = 0.001, loss=sparse_categorical_crossentropy\n",
    "- epochs = 50, batch_size = 32\n",
    "- Use only the first 1000 training samples and first 200 test samples\n",
    "- For MC Dropout, enable dropout during inference and average predictions over 20 stochastic forward passes\n",
    "\n",
    "## Q3.1 Report the test accuracy of the network with Alpha Dropout applied during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "694c0909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.2425 - loss: 2.2238 - val_accuracy: 0.6500 - val_loss: 1.1009\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5462 - loss: 1.4507 - val_accuracy: 0.7950 - val_loss: 0.7066\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6438 - loss: 1.0756 - val_accuracy: 0.8200 - val_loss: 0.5932\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7375 - loss: 0.8364 - val_accuracy: 0.8550 - val_loss: 0.5649\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7650 - loss: 0.6742 - val_accuracy: 0.8550 - val_loss: 0.5568\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8150 - loss: 0.5589 - val_accuracy: 0.8800 - val_loss: 0.5397\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8562 - loss: 0.4717 - val_accuracy: 0.8750 - val_loss: 0.5580\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8675 - loss: 0.4373 - val_accuracy: 0.8650 - val_loss: 0.5807\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8637 - loss: 0.4292 - val_accuracy: 0.8750 - val_loss: 0.6140\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8838 - loss: 0.3728 - val_accuracy: 0.8750 - val_loss: 0.6073\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8838 - loss: 0.3343 - val_accuracy: 0.8950 - val_loss: 0.6073\n",
      "Test accuracy: 0.8500\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, AlphaDropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import save_model\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seeds to 42\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "loss_function = 'sparse_categorical_crossentropy'\n",
    "output_activation = 'softmax'\n",
    "initializer = 'lecun_normal'\n",
    "learning_rate = 0.001\n",
    "hidden_neurons = 64\n",
    "output_neurons = 10\n",
    "activation = 'selu'\n",
    "hidden_layers = 3\n",
    "dropout_rate = 0.1\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "mc_iterations = 20\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Use only the first 1000 training samples and first 200 test samples\n",
    "x_train = x_train[:1000]\n",
    "y_train = y_train[:1000]\n",
    "x_test = x_test[:200]\n",
    "y_test = y_test[:200]\n",
    "\n",
    "# Preprocess the data - flatten and normalize\n",
    "x_train = x_train.reshape(-1, 28 * 28).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 28 * 28).astype('float32') / 255.0\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "\n",
    "# First hidden layer with input shape and alpha Dropout\n",
    "model.add(Dense(hidden_neurons, activation=activation, kernel_initializer=initializer, \n",
    "                input_shape=(28 * 28,)))\n",
    "model.add(AlphaDropout(dropout_rate))\n",
    "\n",
    "# Remaining hidden layers with Alpha Dropout\n",
    "for _ in range(hidden_layers - 1):\n",
    "    model.add(Dense(hidden_neurons, activation=activation, kernel_initializer=initializer))\n",
    "    model.add(AlphaDropout(dropout_rate))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(output_neurons, activation=output_activation))\n",
    "# Compile the model\n",
    "optimizer = Nadam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                     patience=5, \n",
    "                     restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs, \n",
    "                    callbacks=[early_stopping],\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Test accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Save the model\n",
    "save_model(model, 'models/exercise_3_model.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ad418d",
   "metadata": {},
   "source": [
    "## Q3.2 Report the MC Dropout-enhanced accuracy (averaging 20 stochastic predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "decb19a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3.2 MC Dropout-enhanced accuracy: 77.50%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('exercise_3_model.keras')\n",
    "\n",
    "mc_predictions = []\n",
    "# See configuration above for mc_iterations\n",
    "for i in range(mc_iterations):\n",
    "    # Use training=True to enable dropout during inference\n",
    "    predictions = model(x_test, training=True)\n",
    "    # Collect predictions\n",
    "    mc_predictions.append(predictions.numpy())\n",
    "\n",
    "# Average predictions across all MC iterations\n",
    "mc_predictions = np.array(mc_predictions)\n",
    "mc_mean_predictions = np.mean(mc_predictions, axis=0)\n",
    "\n",
    "# Get predicted classes from averaged predictions\n",
    "mc_predicted_classes = np.argmax(mc_mean_predictions, axis=1)\n",
    "\n",
    "# Calculate MC Dropout accuracy\n",
    "mc_accuracy = np.mean(mc_predicted_classes == y_test)\n",
    "print(f'Q3.2 MC Dropout-enhanced accuracy: {mc_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f928bc",
   "metadata": {},
   "source": [
    "# Monte Carlo Dropout\n",
    "The essence of Monte Carlo (MC) dropout is to embrace uncertainy and randomness, hence the name \"Monte Carlo\". \n",
    "\n",
    "During inference (the phase where a trained model is used to make predictions on new, unseen data), instead of turning off dropout (as is standard practice), we keep it active. This means that each time we pass an input through the network, different neurons are randomly dropped out, leading to a variety of outputs for the same input. \n",
    "\n",
    "This stochastic (randomized) behavior allows us to sample from the model's predictive distribution, and allows us to improve the model's performance by averaging these multiple predictions.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envAssignments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
